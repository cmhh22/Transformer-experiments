# ğŸ”¬ BERT vs Qwen: Emotion Classification

## ğŸ“– Overview

This project demonstrates **advanced fine-tuning techniques** for transformer models on emotion classification tasks. We compare two popular architectures â€” **BERT-base-uncased** (Google's bidirectional encoder) and **Qwen2.5-0.5B** (Alibaba's multilingual decoder) â€” on the **6-class Emotion dataset** from HuggingFace. 

The goal is to classify text into emotions: *sadness, joy, love, anger, fear, and surprise*. Through comprehensive experimentation, we evaluate model performance, training efficiency, and provide practical insights for choosing the right model for production scenarios. The project includes complete training pipelines, interpretable metrics, confusion matrices, and an educational Jupyter notebook ready for Google Colab.

---

## ğŸ¯ Project Goals

Fine-tuning **BERT** and **Qwen** transformers for 6-class emotion classification using HuggingFace datasets.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“Š Experimental Results

### Model Comparison

| Model | Accuracy | Precision | Recall | F1 Score | Training Time |
|-------|----------|-----------|--------|----------|---------------|
| **BERT-base** | 92.60% | 92.54% | 92.60% | 92.49% | 5.9 min |
| **Qwen2.5-0.5B** | 92.60% | 92.77% | 92.60% | **92.63%** â­ | 34.3 min |

**ğŸ† Winner:** Qwen (by 0.14% F1) â€” but BERT is **6x faster**

### Per-Emotion F1 Scores

| Emotion | BERT | Qwen | Winner |
|---------|------|------|--------|
| Sadness | 96.65% | 97.29% | Qwen â­ |
| Joy | 93.98% | 93.67% | BERT |
| Love | 79.70% | 79.74% | Tie |
| Anger | 92.13% | 93.33% | Qwen â­ |
| Fear | 90.52% | 89.47% | BERT |
| Surprise | 76.71% | 76.71% | Tie |

### Key Insights
- âš¡ **Speed:** BERT trains 6x faster (ideal for production)
- ğŸ¯ **Accuracy:** Nearly identical performance (0.14% difference)
- ğŸ’¡ **Recommendation:** Use BERT for speed, Qwen for marginal gains on anger/sadness detection

---

## ğŸš€ Quick Start

### Google Colab (Recommended)
1. Upload `notebooks/BERT_vs_Qwen_Emotion_Analysis.ipynb` to Colab
2. Enable GPU: `Runtime > Change runtime type > GPU`
3. Run all cells: `Runtime > Run all`

**No local setup needed** â€” everything runs in the cloud!

### Local Setup (Optional)
```bash
git clone https://github.com/cmhh22/transformer-experiments.git
cd transformer-experiments/BERT-Qwen-Classification
pip install -r requirements.txt
python main.py --model bert --epochs 3
```

---

## ğŸ“ Project Structure

```
BERT-Qwen-Classification/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ BERT_vs_Qwen_Emotion_Analysis.ipynb  # â­ Main notebook (Colab-ready)
â”œâ”€â”€ src/                     # Modular source code
â”‚   â”œâ”€â”€ config.py            # Training configuration
â”‚   â”œâ”€â”€ model.py             # TransformerClassifier architecture
â”‚   â”œâ”€â”€ data_loader.py       # Dataset loading & preprocessing
â”‚   â”œâ”€â”€ train.py             # Training loop with mixed precision
â”‚   â”œâ”€â”€ evaluate.py          # Metrics & visualization
â”‚   â””â”€â”€ utils.py             # Helper utilities
â”œâ”€â”€ main.py                  # CLI for local training
â”œâ”€â”€ test_quick.py            # Quick validation tests
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ LICENSE                  # MIT License
```

---

## ğŸ¯ Features

### Models
| Model | Parameters | Architecture | Pooling Strategy |
|-------|------------|--------------|------------------|
| BERT-base-uncased | ~110M | Encoder-only | CLS token |
| Qwen2.5-0.5B | ~500M | Decoder-only | Mean pooling |

### Dataset
- **Source:** [dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion) (HuggingFace)
- **Classes:** 6 emotions (sadness, joy, love, anger, fear, surprise)
- **Samples:** 16,000 training / 2,000 test
- **Auto-download:** No manual download needed

### Training Features
- âœ… Mixed precision training (FP16)
- âœ… Cosine learning rate scheduling with warmup
- âœ… Gradient clipping for stability
- âœ… Early stopping with best model checkpointing
- âœ… Per-class and weighted metrics

### Visualizations
- ğŸ“Š Training curves (loss & F1)
- ğŸ“Š Confusion matrices
- ğŸ“Š Per-emotion performance comparison
- ğŸ“Š Side-by-side model comparison charts

---

## ğŸ”§ Configuration

Key hyperparameters (modifiable in notebook):

```python
@dataclass
class Config:
    max_length: int = 128      # Max token length
    batch_size: int = 16       # BERT: 16, Qwen: 8
    epochs: int = 3            # Training epochs
    learning_rate: float = 2e-5  # BERT: 2e-5, Qwen: 1e-5
    weight_decay: float = 0.01
    warmup_ratio: float = 0.1
    dropout: float = 0.1
    use_amp: bool = True       # Mixed precision
```

---

## ğŸ“š How It Works

### 1. Data Pipeline
```
HuggingFace Dataset â†’ Tokenization â†’ PyTorch DataLoader â†’ Batches
```

### 2. Model Architecture
```
Input Text â†’ Tokenizer â†’ Transformer (BERT/Qwen) â†’ Pooling â†’ Classification Head â†’ Emotion
```

### 3. Training Loop
```
Forward Pass â†’ Loss (CrossEntropy) â†’ Backward Pass â†’ Optimizer Step â†’ Scheduler Step
```

### 4. Evaluation
```
Predictions â†’ Metrics (Accuracy, F1, Precision, Recall) â†’ Confusion Matrix â†’ Reports
```

---

## ğŸ“ˆ Training Details

### BERT Training
- **Batch size:** 16
- **Learning rate:** 2e-5
- **Pooling:** CLS token (first token)
- **Training time:** ~6 minutes on T4 GPU

### Qwen Training
- **Batch size:** 8 (larger model needs less)
- **Learning rate:** 1e-5 (more conservative for LLM)
- **Pooling:** Mean pooling (no CLS token in decoder models)
- **Training time:** ~34 minutes on T4 GPU

---

## ğŸ› ï¸ CLI Usage (Local)

```bash
# Train BERT
python main.py --model bert --epochs 3 --batch_size 16

# Train with custom dataset
python main.py --model bert --data_path data/custom.csv --text_column text --label_column label

# Evaluate saved model
python main.py --mode eval --model_path models/bert_best.pth

# Interactive prediction
python main.py --mode predict --model_path models/bert_best.pth
```

---

## ğŸ“š References

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Qwen Technical Report](https://arxiv.org/abs/2309.16609)
- [HuggingFace Transformers Documentation](https://huggingface.co/docs/transformers)
- [Emotion Dataset](https://huggingface.co/datasets/dair-ai/emotion)

---

## ğŸ‘¤ Author

**Carlos Manuel HernÃ¡ndez**
- GitHub: [@cmhh22](https://github.com/cmhh22)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- HuggingFace for the Transformers library and Emotion dataset
- Google Colab for free GPU access
- The open-source ML community
