# BERT-Qwen Text Classification

## ğŸ“‹ DescripciÃ³n

Proyecto de clasificaciÃ³n de texto utilizando modelos Transformer (BERT y Qwen). Implementa fine-tuning de modelos preentrenados para tareas de clasificaciÃ³n con anÃ¡lisis comparativo de rendimiento.

## ğŸ¯ Objetivos

- Fine-tuning de BERT para clasificaciÃ³n de texto
- ImplementaciÃ³n de Qwen para tareas de NLP
- AnÃ¡lisis comparativo de modelos Transformer
- EvaluaciÃ³n exhaustiva con mÃ©tricas avanzadas
- VisualizaciÃ³n de resultados y embeddings

## ğŸš€ CaracterÃ­sticas

- **Modelos implementados**: BERT-base, Qwen
- **Arquitectura**: PyTorch + Transformers (HuggingFace)
- **TÃ©cnicas**: Fine-tuning, Transfer Learning, Attention Analysis
- **EvaluaciÃ³n**: Accuracy, F1-Score, Confusion Matrix, ROC-AUC
- **Visualizaciones**: Embeddings t-SNE, Attention Heatmaps

## ğŸ“ Estructura del Proyecto

```
BERT-Qwen-Classification/
â”œâ”€â”€ main.py                 # Script principal de entrenamiento
â”œâ”€â”€ test_quick.py           # Tests rÃ¡pidos
â”œâ”€â”€ requirements.txt        # Dependencias
â”œâ”€â”€ README.md              # DocumentaciÃ³n
â”œâ”€â”€ data/                  # Datasets
â”‚   â”œâ”€â”€ raw/              # Datos originales
â”‚   â””â”€â”€ processed/        # Datos procesados
â”œâ”€â”€ models/               # Modelos guardados
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_bert_training.ipynb
â”‚   â””â”€â”€ 03_qwen_training.ipynb
â””â”€â”€ src/                  # CÃ³digo fuente
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py         # ConfiguraciÃ³n
    â”œâ”€â”€ model.py          # Arquitecturas de modelos
    â”œâ”€â”€ data_loader.py    # Carga de datos
    â”œâ”€â”€ train.py          # LÃ³gica de entrenamiento
    â”œâ”€â”€ evaluate.py       # EvaluaciÃ³n
    â””â”€â”€ utils.py          # Utilidades
```

## ğŸ› ï¸ InstalaciÃ³n

```bash
# Clonar el repositorio
git clone https://github.com/cmhh22/transformer-experiments.git
cd transformer-experiments/BERT-Qwen-Classification

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

## ğŸ“Š Uso

### Entrenamiento

```bash
# Entrenar modelo BERT
python main.py --model bert --epochs 10 --batch_size 32

# Entrenar modelo Qwen
python main.py --model qwen --epochs 10 --batch_size 16
```

### EvaluaciÃ³n

```bash
# Evaluar modelo guardado
python main.py --mode eval --model_path models/best_model.pth
```

### Tests rÃ¡pidos

```bash
python test_quick.py
```

## ğŸ“ˆ Resultados

Los resultados y mÃ©tricas se guardan en:
- `models/`: Modelos entrenados y checkpoints
- `notebooks/`: AnÃ¡lisis detallado y visualizaciones
- MÃ©tricas: Accuracy, F1-Score, Precision, Recall

## ğŸ”§ ConfiguraciÃ³n

Ajusta los hiperparÃ¡metros en `src/config.py`:

```python
LEARNING_RATE = 2e-5
BATCH_SIZE = 32
MAX_LENGTH = 512
NUM_EPOCHS = 10
```

## ğŸ“ Dataset

El proyecto soporta diversos datasets de clasificaciÃ³n de texto:
- IMDB Reviews
- AG News
- Custom datasets (CSV/JSON)

## ğŸ“ Conceptos Clave

- **Transfer Learning**: Aprovechamiento de modelos preentrenados
- **Attention Mechanism**: Mecanismo de atenciÃ³n en Transformers
- **Tokenization**: Procesamiento de texto con tokenizers especÃ­ficos
- **Fine-tuning**: Ajuste de modelos preentrenados

## ğŸ“š Referencias

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Qwen Technical Report](https://arxiv.org/abs/2309.16609)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)

## ğŸ‘¤ Autor

**Carlos HernÃ¡ndez**
- GitHub: [@cmhh22](https://github.com/cmhh22)
- LinkedIn: [Carlos HernÃ¡ndez](https://linkedin.com/in/cmhh22)

## ğŸ“„ Licencia

MIT License - ver archivo [LICENSE](LICENSE) para detalles.

## ğŸš§ Estado del Proyecto

ğŸš€ En desarrollo activo - Enero 2026
