# ğŸ” RAG with ChromaDB

A **Retrieval-Augmented Generation (RAG)** system using ChromaDB as vector database. Designed to run on **Google Colab** with a **100% FREE and UNLIMITED** local LLM.

![RAG Architecture](https://miro.medium.com/v2/resize:fit:1400/1*3q6xmUkB4l5VJv8Q8a8OVA.png)

## ğŸš€ Quick Start on Colab

1. **Open the notebook in Colab:**
   
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/RAG-Chroma/blob/main/RAG_ChromaDB_Colab.ipynb)

2. **Enable GPU:**
   - Go to `Runtime > Change runtime type > T4 GPU`

3. **Run the notebook cells in order**

> âš¡ **No API key needed!** The model runs locally on Colab's free GPU.

## ğŸ“ Project Structure

```
RAG-Chroma/
â”œâ”€â”€ ğŸ““ RAG_ChromaDB_Colab.ipynb  # Main notebook
â”œâ”€â”€ ğŸ“ data/                      # Documents to index
â”‚   â”œâ”€â”€ machine_learning.txt      # ML fundamentals
â”‚   â”œâ”€â”€ deep_learning.txt         # Neural networks
â”‚   â”œâ”€â”€ transformers.txt          # Transformer architecture
â”‚   â””â”€â”€ rag_systems.txt           # RAG systems
â”œâ”€â”€ ğŸ“„ requirements.txt           # Dependencies
â””â”€â”€ ğŸ“„ README.md                  # This file
```

## ğŸ› ï¸ Technologies

| Component | Technology |
|-----------|------------|
| Vector Store | ChromaDB |
| Embeddings | Sentence Transformers (all-MiniLM-L6-v2) |
| Orchestration | LangChain |
| LLM | TinyLlama 1.1B (Local, FREE, Unlimited) |
| Document Loaders | PyPDF, python-docx |

## ğŸ“Š Features

- âœ… **Multiple formats**: TXT, PDF, DOCX
- âœ… **Smart chunking**: Recursive character splitting
- âœ… **Free embeddings**: Sentence Transformers (local)
- âœ… **FREE & Unlimited LLM**: Runs locally on GPU
- âœ… **Persistence**: ChromaDB persists to disk
- âœ… **Source citation**: Shows where information comes from
- âœ… **Interactive chat**: Q&A interface in notebook

## ğŸ¯ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INDEXING (Offline)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Documents â†’ Chunking â†’ Embeddings â†’ ChromaDB               â”‚
â”‚     ğŸ“„          âœ‚ï¸          ğŸ”¢           ğŸ’¾                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QUERY (Online)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Question â†’ Embedding â†’ Search â†’ Context â†’ LLM â†’ Answer      â”‚
â”‚     â“          ğŸ”¢         ğŸ”        ğŸ“š      ğŸ¤–      ğŸ’¬       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Add Your Own Documents

### Option 1: Upload in Colab
Run the "Upload files" cell and select your documents.

### Option 2: Local data/ folder
Place your files in the `data/` folder before uploading to Colab:
- `.txt` - Text files
- `.pdf` - PDF documents
- `.docx` - Word documents

### Option 3: Google Drive
```python
from google.colab import drive
drive.mount('/content/drive')
# Then use: load_documents('/content/drive/MyDrive/my_documents')
```

## âš™ï¸ Configuration

### Adjust Chunking
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # Increase for longer documents
    chunk_overlap=50,    # Overlap between chunks
)
```

### Change Number of Retrieved Documents
```python
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 5}  # Retrieve top 5 documents
)
```

## ğŸ”¬ RAG Pipeline Explained

1. **Document Loading**: Load documents from various formats (TXT, PDF, DOCX)
2. **Text Splitting**: Divide documents into manageable chunks
3. **Embedding Generation**: Convert text chunks to vector representations
4. **Vector Storage**: Store embeddings in ChromaDB
5. **Semantic Search**: Find relevant chunks based on query similarity
6. **Answer Generation**: Use LLM to generate answers from retrieved context

## ğŸ“ˆ Performance Tips

- Enable GPU for faster inference: `Runtime > Change runtime type > T4 GPU`
- Reduce `k` in retriever for faster responses
- Adjust `chunk_size` based on your document types
- Use smaller embedding models for faster indexing

## ğŸ¤ Contributing

Feel free to open issues or submit pull requests!

## ğŸ“„ License

MIT License - Feel free to use this project for learning and development.

---

**Built with â¤ï¸ using LangChain, ChromaDB, and Hugging Face Transformers**
