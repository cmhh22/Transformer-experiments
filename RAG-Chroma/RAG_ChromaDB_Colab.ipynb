{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea6ce4c",
   "metadata": {},
   "source": [
    "# üîç RAG with ChromaDB - Question & Answer System\n",
    "\n",
    "This notebook implements a **RAG (Retrieval-Augmented Generation)** system using:\n",
    "- **ChromaDB** as vector database\n",
    "- **Sentence Transformers** for embeddings (FREE)\n",
    "- **LangChain** for orchestration\n",
    "- **TinyLlama** - 100% FREE & UNLIMITED local LLM (runs on Colab GPU)\n",
    "\n",
    "> ‚ö° **No API key needed!** Everything runs locally on Colab's free GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f6b9a",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1558396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Installation completed\n",
      "‚ö†Ô∏è IMPORTANT: Make sure GPU is enabled:\n",
      "   Runtime > Change runtime type > T4 GPU\n"
     ]
    }
   ],
   "source": [
    "# Install libraries - 100% FREE & UNLIMITED\n",
    "# The model runs locally on Colab's GPU (no API key needed)\n",
    "!pip install -q --upgrade langchain langchain-core langchain-community\n",
    "!pip install -q langchain-text-splitters\n",
    "!pip install -q chromadb sentence-transformers\n",
    "!pip install -q pypdf python-docx unstructured\n",
    "!pip install -q transformers accelerate bitsandbytes  # For local model\n",
    "!pip install -q tiktoken\n",
    "\n",
    "print(\"‚úÖ Installation completed\")\n",
    "print(\"‚ö†Ô∏è IMPORTANT: Make sure GPU is enabled:\")\n",
    "print(\"   Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27292fde",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Setup and Create Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25de4e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Creando documentos de ejemplo...\n",
      "\n",
      "‚úÖ Creado: data/machine_learning.txt\n",
      "‚úÖ Creado: data/deep_learning.txt\n",
      "‚úÖ Creado: data/transformers.txt\n",
      "‚úÖ Creado: data/rag_systems.txt\n",
      "\n",
      "üìä Total documentos: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Crear carpeta data si no existe\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Documentos de ejemplo sobre ML/AI\n",
    "SAMPLE_DOCS = {\n",
    "    \"machine_learning.txt\": '''# Machine Learning - Fundamentos\n",
    "\n",
    "Machine Learning (ML) es una rama de la inteligencia artificial que permite a los sistemas aprender y mejorar autom√°ticamente a partir de la experiencia sin ser programados expl√≠citamente.\n",
    "\n",
    "## Tipos de Aprendizaje\n",
    "\n",
    "### Aprendizaje Supervisado\n",
    "El modelo aprende de datos etiquetados. Ejemplos:\n",
    "- Clasificaci√≥n: Spam detection, diagn√≥stico m√©dico\n",
    "- Regresi√≥n: Predicci√≥n de precios, pron√≥stico del tiempo\n",
    "\n",
    "Algoritmos populares:\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Support Vector Machines (SVM)\n",
    "- Gradient Boosting (XGBoost, LightGBM)\n",
    "\n",
    "### Aprendizaje No Supervisado\n",
    "El modelo encuentra patrones en datos sin etiquetar:\n",
    "- Clustering: K-Means, DBSCAN, Hierarchical\n",
    "- Reducci√≥n de dimensionalidad: PCA, t-SNE, UMAP\n",
    "- Detecci√≥n de anomal√≠as: Isolation Forest\n",
    "\n",
    "### Aprendizaje por Refuerzo\n",
    "El agente aprende mediante interacci√≥n con el entorno:\n",
    "- Q-Learning\n",
    "- Deep Q-Networks (DQN)\n",
    "- Policy Gradient Methods\n",
    "- Actor-Critic (A2C, A3C, PPO)\n",
    "\n",
    "## Proceso de ML\n",
    "\n",
    "1. Recolecci√≥n de datos: Obtener datos relevantes\n",
    "2. Preprocesamiento: Limpieza, normalizaci√≥n, encoding\n",
    "3. Feature Engineering: Crear caracter√≠sticas √∫tiles\n",
    "4. Entrenamiento: Ajustar el modelo a los datos\n",
    "5. Evaluaci√≥n: M√©tricas como accuracy, precision, recall, F1\n",
    "6. Deployment: Poner el modelo en producci√≥n\n",
    "\n",
    "## M√©tricas de Evaluaci√≥n\n",
    "\n",
    "### Clasificaci√≥n\n",
    "- Accuracy: Proporci√≥n de predicciones correctas\n",
    "- Precision: TP / (TP + FP)\n",
    "- Recall: TP / (TP + FN)\n",
    "- F1-Score: Media arm√≥nica de precision y recall\n",
    "- AUC-ROC: √Årea bajo la curva ROC\n",
    "\n",
    "### Regresi√≥n\n",
    "- MSE (Mean Squared Error)\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- R2 (Coefficient of Determination)\n",
    "''',\n",
    "\n",
    "    \"deep_learning.txt\": '''# Deep Learning - Redes Neuronales Profundas\n",
    "\n",
    "Deep Learning es un subconjunto de Machine Learning basado en redes neuronales artificiales con m√∫ltiples capas.\n",
    "\n",
    "## Fundamentos\n",
    "\n",
    "### Neurona Artificial (Perceptr√≥n)\n",
    "- Recibe entradas ponderadas\n",
    "- Aplica una funci√≥n de activaci√≥n\n",
    "- Produce una salida\n",
    "\n",
    "### Funciones de Activaci√≥n\n",
    "- ReLU: max(0, x) - M√°s usada en capas ocultas\n",
    "- Sigmoid: 1/(1+e^-x) - Salidas entre 0 y 1\n",
    "- Tanh: Salidas entre -1 y 1\n",
    "- Softmax: Para clasificaci√≥n multiclase\n",
    "- GELU: Usada en Transformers\n",
    "\n",
    "## Arquitecturas Principales\n",
    "\n",
    "### Redes Feedforward (MLP)\n",
    "- Capas densamente conectadas\n",
    "- Informaci√≥n fluye en una direcci√≥n\n",
    "- √ötil para datos tabulares\n",
    "\n",
    "### Redes Convolucionales (CNN)\n",
    "- Especializadas en procesamiento de im√°genes\n",
    "- Capas convolucionales detectan patrones locales\n",
    "- Pooling reduce dimensionalidad\n",
    "- Arquitecturas: LeNet, AlexNet, VGG, ResNet, EfficientNet\n",
    "\n",
    "### Redes Recurrentes (RNN)\n",
    "- Procesan secuencias de datos\n",
    "- Mantienen estado interno (memoria)\n",
    "- Variantes: LSTM, GRU\n",
    "- Problema: Vanishing gradients en secuencias largas\n",
    "\n",
    "### Transformers\n",
    "- Arquitectura dominante actual\n",
    "- Mecanismo de atenci√≥n (self-attention)\n",
    "- Procesamiento paralelo\n",
    "- Base de GPT, BERT, T5, etc.\n",
    "\n",
    "## Entrenamiento\n",
    "\n",
    "### Backpropagation\n",
    "- Calcula gradientes de la funci√≥n de p√©rdida\n",
    "- Propaga errores hacia atr√°s\n",
    "- Actualiza pesos con optimizador\n",
    "\n",
    "### Optimizadores\n",
    "- SGD: Stochastic Gradient Descent\n",
    "- Adam: Adaptive Moment Estimation (m√°s popular)\n",
    "- AdamW: Adam con weight decay\n",
    "- RMSprop: Para RNNs\n",
    "\n",
    "### Regularizaci√≥n\n",
    "- Dropout: Desactiva neuronas aleatoriamente\n",
    "- Batch Normalization: Normaliza activaciones\n",
    "- Layer Normalization: Usada en Transformers\n",
    "- Weight Decay (L2): Penaliza pesos grandes\n",
    "\n",
    "## Frameworks\n",
    "- PyTorch: Flexible, preferido en investigaci√≥n\n",
    "- TensorFlow/Keras: Producci√≥n, TensorFlow Serving\n",
    "- JAX: Diferenciaci√≥n autom√°tica, XLA\n",
    "- Hugging Face: Modelos pre-entrenados de NLP\n",
    "''',\n",
    "\n",
    "    \"transformers.txt\": '''# Transformers - Arquitectura Revolucionaria\n",
    "\n",
    "La arquitectura Transformer, introducida en \"Attention is All You Need\" (2017) por Vaswani et al., revolucion√≥ el procesamiento de lenguaje natural.\n",
    "\n",
    "## Componentes Clave\n",
    "\n",
    "### Self-Attention (Atenci√≥n)\n",
    "Permite que cada token \"atienda\" a todos los dem√°s tokens:\n",
    "\n",
    "1. Query (Q): Lo que el token busca\n",
    "2. Key (K): Lo que cada token ofrece\n",
    "3. Value (V): La informaci√≥n real\n",
    "\n",
    "F√≥rmula: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "\n",
    "### Multi-Head Attention\n",
    "- M√∫ltiples cabezas de atenci√≥n en paralelo\n",
    "- Cada cabeza aprende diferentes patrones\n",
    "- Se concatenan y proyectan\n",
    "\n",
    "### Positional Encoding\n",
    "- Transformers no tienen noci√≥n de orden\n",
    "- Se a√±ade informaci√≥n posicional\n",
    "- Encodings sinusoidales o aprendidos\n",
    "\n",
    "### Feed-Forward Network (FFN)\n",
    "- Dos capas lineales con activaci√≥n\n",
    "- Procesa cada posici√≥n independientemente\n",
    "- Expande y contrae dimensionalidad\n",
    "\n",
    "## Arquitecturas Derivadas\n",
    "\n",
    "### Encoder-Only (BERT-style)\n",
    "- Bidireccional, ve todo el contexto\n",
    "- Ideal para clasificaci√≥n, NER, embeddings\n",
    "- Ejemplos: BERT, RoBERTa, ALBERT, DistilBERT\n",
    "\n",
    "### Decoder-Only (GPT-style)\n",
    "- Autoregresivo, genera token por token\n",
    "- Ideal para generaci√≥n de texto\n",
    "- Ejemplos: GPT-2, GPT-3, GPT-4, LLaMA, Mistral\n",
    "\n",
    "### Encoder-Decoder (T5-style)\n",
    "- Encoder procesa entrada\n",
    "- Decoder genera salida\n",
    "- Ideal para traducci√≥n, summarization\n",
    "- Ejemplos: T5, BART, mT5\n",
    "\n",
    "## Large Language Models (LLMs)\n",
    "\n",
    "### Caracter√≠sticas\n",
    "- Billones de par√°metros\n",
    "- Entrenados en enormes corpus de texto\n",
    "- Capacidades emergentes (few-shot, reasoning)\n",
    "\n",
    "### T√©cnicas de Entrenamiento\n",
    "- Pre-training: Modelado de lenguaje en corpus grande\n",
    "- Fine-tuning: Ajuste para tarea espec√≠fica\n",
    "- RLHF: Reinforcement Learning from Human Feedback\n",
    "- Instruction Tuning: Entrenamiento con instrucciones\n",
    "\n",
    "### Optimizaciones\n",
    "- LoRA: Low-Rank Adaptation (fine-tuning eficiente)\n",
    "- QLoRA: LoRA con cuantizaci√≥n\n",
    "- Flash Attention: Atenci√≥n m√°s r√°pida y eficiente\n",
    "- Quantization: INT8, INT4 para menor memoria\n",
    "''',\n",
    "\n",
    "    \"rag_systems.txt\": '''# RAG - Retrieval-Augmented Generation\n",
    "\n",
    "RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje para crear sistemas que pueden responder preguntas bas√°ndose en documentos espec√≠ficos.\n",
    "\n",
    "## Por qu√© RAG?\n",
    "\n",
    "### Limitaciones de LLMs puros\n",
    "- Conocimiento desactualizado (fecha de corte)\n",
    "- No acceso a informaci√≥n privada/propietaria\n",
    "- Alucinaciones: inventan informaci√≥n falsa\n",
    "- No pueden citar fuentes espec√≠ficas\n",
    "\n",
    "### Ventajas de RAG\n",
    "- Informaci√≥n actualizada en tiempo real\n",
    "- Acceso a documentos privados\n",
    "- Respuestas fundamentadas en fuentes\n",
    "- Reducci√≥n de alucinaciones\n",
    "- Trazabilidad y citaci√≥n\n",
    "\n",
    "## Arquitectura RAG\n",
    "\n",
    "### 1. Indexaci√≥n (Offline)\n",
    "1. Carga de documentos: PDFs, TXT, DOCX, HTML, etc.\n",
    "2. Chunking: Dividir en fragmentos manejables\n",
    "   - Por caracteres/tokens\n",
    "   - Por oraciones/p√°rrafos\n",
    "   - Recursive character splitting\n",
    "   - Semantic chunking\n",
    "3. Embedding: Convertir texto a vectores\n",
    "   - Sentence Transformers\n",
    "   - Cohere Embeddings\n",
    "4. Almacenamiento: Guardar en vector store\n",
    "   - ChromaDB, FAISS, Pinecone, Weaviate, Milvus\n",
    "\n",
    "### 2. Recuperaci√≥n (Online)\n",
    "1. Query embedding: Convertir pregunta a vector\n",
    "2. Similarity search: Buscar chunks similares\n",
    "   - Cosine similarity\n",
    "   - Euclidean distance\n",
    "   - Dot product\n",
    "3. Reranking (opcional): Reordenar resultados\n",
    "\n",
    "### 3. Generaci√≥n\n",
    "1. Prompt construction: Crear prompt con contexto\n",
    "2. LLM call: Generar respuesta\n",
    "3. Post-processing: Formatear, citar fuentes\n",
    "\n",
    "## T√©cnicas Avanzadas\n",
    "\n",
    "### Hybrid Search\n",
    "- Combina b√∫squeda sem√°ntica + keyword (BM25)\n",
    "- Mejor cobertura de consultas\n",
    "\n",
    "### Multi-Query RAG\n",
    "- Genera m√∫ltiples variantes de la pregunta\n",
    "- Recupera documentos para cada variante\n",
    "- Fusiona resultados\n",
    "\n",
    "### Self-RAG\n",
    "- El modelo decide cu√°ndo recuperar\n",
    "- Eval√∫a relevancia de documentos\n",
    "- Auto-cr√≠tica de respuestas\n",
    "\n",
    "### Agentic RAG\n",
    "- Agentes que deciden qu√© herramientas usar\n",
    "- Pueden iterar y refinar b√∫squedas\n",
    "- Routing entre m√∫ltiples fuentes\n",
    "\n",
    "## Vector Stores\n",
    "\n",
    "### ChromaDB\n",
    "- Open source, f√°cil de usar\n",
    "- Perfecto para desarrollo y prototipos\n",
    "- Persistencia local o en memoria\n",
    "\n",
    "### FAISS (Facebook)\n",
    "- Muy r√°pido, optimizado para GPU\n",
    "- Ideal para grandes vol√∫menes\n",
    "\n",
    "### Pinecone\n",
    "- Servicio cloud gestionado\n",
    "- Escalable, alta disponibilidad\n",
    "\n",
    "### Weaviate\n",
    "- B√∫squeda h√≠brida nativa\n",
    "- GraphQL API\n",
    "\n",
    "## M√©tricas de Evaluaci√≥n\n",
    "\n",
    "### Retrieval\n",
    "- Precision@K\n",
    "- Recall@K\n",
    "- MRR (Mean Reciprocal Rank)\n",
    "- NDCG\n",
    "\n",
    "### Generation\n",
    "- Faithfulness: Respuesta fiel al contexto?\n",
    "- Answer Relevance: Responde la pregunta?\n",
    "- Context Relevance: Contexto relevante?\n",
    "- Frameworks: RAGAS, TruLens\n",
    "'''\n",
    "}\n",
    "\n",
    "# Guardar documentos de ejemplo\n",
    "print(\"üìÅ Creando documentos de ejemplo...\\n\")\n",
    "for filename, content in SAMPLE_DOCS.items():\n",
    "    filepath = DATA_DIR / filename\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"‚úÖ Creado: {filepath}\")\n",
    "\n",
    "print(f\"\\nüìä Total documentos: {len(list(DATA_DIR.glob('*')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b2311",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configuration - 100% FREE & UNLIMITED\n",
    "\n",
    "This system uses a **local LLM** that runs on Colab's free GPU.\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Model** | TinyLlama 1.1B Chat |\n",
    "| **Cost** | 100% FREE |\n",
    "| **Limits** | UNLIMITED |\n",
    "| **API Key** | NOT REQUIRED |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0259c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU available: Tesla T4\n",
      "   Memory: 15.8 GB\n",
      "\n",
      "‚úÖ Configured for LOCAL model - 100% FREE & UNLIMITED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# ============================================\n",
    "# üéØ CONFIGURATION - 100% FREE & UNLIMITED\n",
    "# ============================================\n",
    "# Using a LOCAL model that runs on Colab's GPU\n",
    "# No API key needed, no limits, completely free\n",
    "# ============================================\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not detected. Go to: Runtime > Change runtime type > T4 GPU\")\n",
    "\n",
    "LLM_PROVIDER = \"local\"  # Local model, no API\n",
    "print(\"\\n‚úÖ Configured for LOCAL model - 100% FREE & UNLIMITED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447ddbb",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04638c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading documents...\n",
      "\n",
      "üìÑ Loaded 4 .txt files\n",
      "üìÑ Loaded 0 .pdf files\n",
      "üìÑ Loaded 0 .docx files\n",
      "\n",
      "‚úÖ Total documents loaded: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_documents(data_path: str = \"data\"):\n",
    "    \"\"\"Load documents from multiple formats in a folder.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Load .txt files\n",
    "    txt_loader = DirectoryLoader(\n",
    "        data_path, \n",
    "        glob=\"**/*.txt\", \n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    try:\n",
    "        txt_docs = txt_loader.load()\n",
    "        documents.extend(txt_docs)\n",
    "        print(f\"üìÑ Loaded {len(txt_docs)} .txt files\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading .txt: {e}\")\n",
    "    \n",
    "    # Load .pdf files\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        data_path, \n",
    "        glob=\"**/*.pdf\", \n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    try:\n",
    "        pdf_docs = pdf_loader.load()\n",
    "        documents.extend(pdf_docs)\n",
    "        print(f\"üìÑ Loaded {len(pdf_docs)} .pdf files\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading .pdf: {e}\")\n",
    "    \n",
    "    # Load .docx files\n",
    "    docx_loader = DirectoryLoader(\n",
    "        data_path, \n",
    "        glob=\"**/*.docx\", \n",
    "        loader_cls=Docx2txtLoader\n",
    "    )\n",
    "    try:\n",
    "        docx_docs = docx_loader.load()\n",
    "        documents.extend(docx_docs)\n",
    "        print(f\"üìÑ Loaded {len(docx_docs)} .docx files\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading .docx: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "print(\"üìö Loading documents...\\n\")\n",
    "documents = load_documents(\"data\")\n",
    "print(f\"\\n‚úÖ Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14cc1f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chunking Statistics:\n",
      "   ‚Ä¢ Original documents: 4\n",
      "   ‚Ä¢ Generated chunks: 22\n",
      "   ‚Ä¢ Average size: 372 characters\n",
      "\n",
      "üìù Sample chunk:\n",
      "==================================================\n",
      "# RAG - Retrieval-Augmented Generation\n",
      "\n",
      "RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje para crear sistemas que pueden responder preguntas bas√°ndose en documentos espec√≠ficos.\n",
      "\n",
      "## Por qu√© RAG?\n",
      "\n",
      "### Limitaciones de LLMs puros\n",
      "- Conocimiento desactualizado (fecha de corte)\n",
      "- No acce\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"üìä Chunking Statistics:\")\n",
    "print(f\"   ‚Ä¢ Original documents: {len(documents)}\")\n",
    "print(f\"   ‚Ä¢ Generated chunks: {len(chunks)}\")\n",
    "if chunks:\n",
    "    print(f\"   ‚Ä¢ Average size: {sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
    "    print(f\"\\nüìù Sample chunk:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(chunks[0].page_content[:300])\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11b7f9",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create Vector Database with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b32cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading embedding model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81781ce779fa48db8ccd0cdbba34a9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded\n",
      "\n",
      "üîÑ Creating vector database...\n",
      "‚úÖ ChromaDB created with 44 vectors\n",
      "üìÅ Persisted to: chroma_db/\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Configure embedding model (free, local)\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"‚úÖ Embedding model loaded\")\n",
    "\n",
    "# Directory to persist ChromaDB\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "\n",
    "# Create vector store with ChromaDB\n",
    "print(\"\\nüîÑ Creating vector database...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    collection_name=\"rag_collection\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ChromaDB created with {vectorstore._collection.count()} vectors\")\n",
    "print(f\"üìÅ Persisted to: {CHROMA_PATH}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23040939",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1089d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'What is attention in transformers?'\n",
      "\n",
      "üìö Top 3 results:\n",
      "\n",
      "============================================================\n",
      "üìÑ Result #1 (Score: 0.9635)\n",
      "üìÅ Source: data/transformers.txt\n",
      "============================================================\n",
      "# Transformers - Arquitectura Revolucionaria\n",
      "\n",
      "La arquitectura Transformer, introducida en \"Attention is All You Need\" (2017) por Vaswani et al., revolucion√≥ el procesamiento de lenguaje natural.\n",
      "\n",
      "## Componentes Clave\n",
      "\n",
      "### Self-Attention (Atenci√≥n)\n",
      "Permite que cada token \"atienda\" a todos los dem√°s tokens:\n",
      "\n",
      "1. Query (Q): Lo que el token busca\n",
      "2. Key (K): Lo que cada token ofrece\n",
      "3. Value (V): La in\n",
      "\n",
      "============================================================\n",
      "üìÑ Result #2 (Score: 0.9635)\n",
      "üìÅ Source: data/transformers.txt\n",
      "============================================================\n",
      "# Transformers - Arquitectura Revolucionaria\n",
      "\n",
      "La arquitectura Transformer, introducida en \"Attention is All You Need\" (2017) por Vaswani et al., revolucion√≥ el procesamiento de lenguaje natural.\n",
      "\n",
      "## Componentes Clave\n",
      "\n",
      "### Self-Attention (Atenci√≥n)\n",
      "Permite que cada token \"atienda\" a todos los dem√°s tokens:\n",
      "\n",
      "1. Query (Q): Lo que el token busca\n",
      "2. Key (K): Lo que cada token ofrece\n",
      "3. Value (V): La in\n",
      "\n",
      "============================================================\n",
      "üìÑ Result #3 (Score: 1.3723)\n",
      "üìÅ Source: data/deep_learning.txt\n",
      "============================================================\n",
      "### Transformers\n",
      "- Arquitectura dominante actual\n",
      "- Mecanismo de atenci√≥n (self-attention)\n",
      "- Procesamiento paralelo\n",
      "- Base de GPT, BERT, T5, etc.\n",
      "\n",
      "## Entrenamiento\n",
      "\n",
      "### Backpropagation\n",
      "- Calcula gradientes de la funci√≥n de p√©rdida\n",
      "- Propaga errores hacia atr√°s\n",
      "- Actualiza pesos con optimizador\n",
      "\n",
      "### Optimizadores\n",
      "- SGD: Stochastic Gradient Descent\n",
      "- Adam: Adaptive Moment Estimation (m√°s popular)\n",
      "- A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_similar(query: str, k: int = 3):\n",
    "    \"\"\"Search for the k most similar documents to the query.\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    print(f\"üîç Query: '{query}'\\n\")\n",
    "    print(f\"üìö Top {k} results:\\n\")\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üìÑ Result #{i} (Score: {score:.4f})\")\n",
    "        print(f\"üìÅ Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(doc.page_content[:400])\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "results = search_similar(\"What is attention in transformers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1fd9e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'How does RAG work and what are its advantages?'\n",
      "\n",
      "üìö Top 3 results:\n",
      "\n",
      "============================================================\n",
      "üìÑ Result #1 (Score: 1.1671)\n",
      "üìÅ Source: data/rag_systems.txt\n",
      "============================================================\n",
      "### Ventajas de RAG\n",
      "- Informaci√≥n actualizada en tiempo real\n",
      "- Acceso a documentos privados\n",
      "- Respuestas fundamentadas en fuentes\n",
      "- Reducci√≥n de alucinaciones\n",
      "- Trazabilidad y citaci√≥n\n",
      "\n",
      "## Arquitectura RAG\n",
      "\n",
      "============================================================\n",
      "üìÑ Result #2 (Score: 1.1671)\n",
      "üìÅ Source: data/rag_systems.txt\n",
      "============================================================\n",
      "### Ventajas de RAG\n",
      "- Informaci√≥n actualizada en tiempo real\n",
      "- Acceso a documentos privados\n",
      "- Respuestas fundamentadas en fuentes\n",
      "- Reducci√≥n de alucinaciones\n",
      "- Trazabilidad y citaci√≥n\n",
      "\n",
      "## Arquitectura RAG\n",
      "\n",
      "============================================================\n",
      "üìÑ Result #3 (Score: 1.3461)\n",
      "üìÅ Source: data/rag_systems.txt\n",
      "============================================================\n",
      "# RAG - Retrieval-Augmented Generation\n",
      "\n",
      "RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje para crear sistemas que pueden responder preguntas bas√°ndose en documentos espec√≠ficos.\n",
      "\n",
      "## Por qu√© RAG?\n",
      "\n",
      "### Limitaciones de LLMs puros\n",
      "- Conocimiento desactualizado (fecha de corte)\n",
      "- No acceso a informaci√≥n privada/propietaria\n",
      "- Alucinaciones: inventan informaci√≥n falsa\n",
      "- No pueden citar f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More search examples\n",
    "results = search_similar(\"How does RAG work and what are its advantages?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd4b55",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Configure LLM and Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87a23336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model (may take 1-2 minutes the first time)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e71118e6bed479b87bc36bdeaf30f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "üéâ 100% FREE & UNLIMITED - No API key, no limits\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# ============================================\n",
    "# Load LOCAL model - 100% FREE & UNLIMITED\n",
    "# ============================================\n",
    "print(\"üîÑ Loading model (may take 1-2 minutes the first time)...\")\n",
    "\n",
    "# Using TinyLlama - lightweight but effective, works well on Colab\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Create generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Wrapper for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model_id}\")\n",
    "print(\"üéâ 100% FREE & UNLIMITED - No API key, no limits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b9257b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain configured - 100% LOCAL and FREE\n"
     ]
    }
   ],
   "source": [
    "# Create RAG prompt template (optimized for local model)\n",
    "RAG_TEMPLATE = \"\"\"<|system|>\n",
    "You are an expert assistant that answers questions based ONLY on the provided context.\n",
    "Answer concisely and accurately. If the information is not in the context, say \"I don't have enough information to answer\".\n",
    "</s>\n",
    "<|user|>\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT = PromptTemplate(\n",
    "    template=RAG_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"[{source}]: {doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Reduced to 3 for better performance\n",
    ")\n",
    "\n",
    "# Create RAG chain with LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | RAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain configured - 100% LOCAL and FREE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587480e2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Ask Questions! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d3c1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str, show_sources: bool = True):\n",
    "    \"\"\"Function to ask questions to the RAG system.\"\"\"\n",
    "    print(f\"‚ùì Question: {question}\\n\")\n",
    "    print(\"üîÑ Processing...\\n\")\n",
    "    \n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    print(\"üí¨ Answer:\")\n",
    "    print(\"=\"*60)\n",
    "    print(response)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if show_sources:\n",
    "        docs = retriever.invoke(question)\n",
    "        print(\"\\nüìö Sources used:\")\n",
    "        sources = set(doc.metadata.get('source', 'N/A') for doc in docs)\n",
    "        for source in sources:\n",
    "            print(f\"   ‚Ä¢ {source}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "573e70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What is the attention mechanism in Transformers and how does it work?\n",
      "\n",
      "üîÑ Processing...\n",
      "\n",
      "üí¨ Answer:\n",
      "============================================================\n",
      "<|system|>\n",
      "You are an expert assistant that answers questions based ONLY on the provided context.\n",
      "Answer concisely and accurately. If the information is not in the context, say \"I don't have enough information to answer\".\n",
      "</s>\n",
      "<|user|>\n",
      "CONTEXT:\n",
      "[data/transformers.txt]: # Transformers - Arquitectura Revolucionaria\n",
      "\n",
      "La arquitectura Transformer, introducida en \"Attention is All You Need\" (2017) por Vaswani et al., revolucion√≥ el procesamiento de lenguaje natural.\n",
      "\n",
      "## Componentes Clave\n",
      "\n",
      "### Self-Attention (Atenci√≥n)\n",
      "Permite que cada token \"atienda\" a todos los dem√°s tokens:\n",
      "\n",
      "1. Query (Q): Lo que el token busca\n",
      "2. Key (K): Lo que cada token ofrece\n",
      "3. Value (V): La informaci√≥n real\n",
      "\n",
      "F√≥rmula: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
      "\n",
      "[data/transformers.txt]: # Transformers - Arquitectura Revolucionaria\n",
      "\n",
      "La arquitectura Transformer, introducida en \"Attention is All You Need\" (2017) por Vaswani et al., revolucion√≥ el procesamiento de lenguaje natural.\n",
      "\n",
      "## Componentes Clave\n",
      "\n",
      "### Self-Attention (Atenci√≥n)\n",
      "Permite que cada token \"atienda\" a todos los dem√°s tokens:\n",
      "\n",
      "1. Query (Q): Lo que el token busca\n",
      "2. Key (K): Lo que cada token ofrece\n",
      "3. Value (V): La informaci√≥n real\n",
      "\n",
      "F√≥rmula: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
      "\n",
      "[data/deep_learning.txt]: ### Transformers\n",
      "- Arquitectura dominante actual\n",
      "- Mecanismo de atenci√≥n (self-attention)\n",
      "- Procesamiento paralelo\n",
      "- Base de GPT, BERT, T5, etc.\n",
      "\n",
      "## Entrenamiento\n",
      "\n",
      "### Backpropagation\n",
      "- Calcula gradientes de la funci√≥n de p√©rdida\n",
      "- Propaga errores hacia atr√°s\n",
      "- Actualiza pesos con optimizador\n",
      "\n",
      "### Optimizadores\n",
      "- SGD: Stochastic Gradient Descent\n",
      "- Adam: Adaptive Moment Estimation (m√°s popular)\n",
      "- AdamW: Adam con weight decay\n",
      "- RMSprop: Para RNNs\n",
      "\n",
      "QUESTION: What is the attention mechanism in Transformers and how does it work?\n",
      "</s>\n",
      "<|assistant|>\n",
      "The attention mechanism in Transformers is called self-attention (attenci√≥n), which allows each token to attend to all other tokens in the input sequence by calculating a weighted sum of their own values and those of the corresponding keys. The formula for this operation is Attention(Q, K, V) = Softmax(QK^T / sqrt(d_k)) * V, where Q is the query vector, K is the key vector, and V is the value vector. This process helps to identify relevant information from the input sequence and focuses on the most important parts of the text. By attending to only the relevant elements, Transformers can improve the performance of various tasks such as language modeling, question answering, and machine translation. Overall, the attention mechanism plays a crucial role in transformer architectures, making them one of the most successful deep learning models currently available.\n",
      "============================================================\n",
      "\n",
      "üìö Sources used:\n",
      "   ‚Ä¢ data/transformers.txt\n",
      "   ‚Ä¢ data/deep_learning.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|system|>\\nYou are an expert assistant that answers questions based ONLY on the provided context.\\nAnswer concisely and accurately. If the information is not in the context, say \"I don\\'t have enough information to answer\".\\n</s>\\n<|user|>\\nCONTEXT:\\n[data/transformers.txt]: # Transformers - Arquitectura Revolucionaria\\n\\nLa arquitectura Transformer, introducida en \"Attention is All You Need\" (2017) por Vaswani et al., revolucion√≥ el procesamiento de lenguaje natural.\\n\\n## Componentes Clave\\n\\n### Self-Attention (Atenci√≥n)\\nPermite que cada token \"atienda\" a todos los dem√°s tokens:\\n\\n1. Query (Q): Lo que el token busca\\n2. Key (K): Lo que cada token ofrece\\n3. Value (V): La informaci√≥n real\\n\\nF√≥rmula: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\\n\\n[data/transformers.txt]: # Transformers - Arquitectura Revolucionaria\\n\\nLa arquitectura Transformer, introducida en \"Attention is All You Need\" (2017) por Vaswani et al., revolucion√≥ el procesamiento de lenguaje natural.\\n\\n## Componentes Clave\\n\\n### Self-Attention (Atenci√≥n)\\nPermite que cada token \"atienda\" a todos los dem√°s tokens:\\n\\n1. Query (Q): Lo que el token busca\\n2. Key (K): Lo que cada token ofrece\\n3. Value (V): La informaci√≥n real\\n\\nF√≥rmula: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\\n\\n[data/deep_learning.txt]: ### Transformers\\n- Arquitectura dominante actual\\n- Mecanismo de atenci√≥n (self-attention)\\n- Procesamiento paralelo\\n- Base de GPT, BERT, T5, etc.\\n\\n## Entrenamiento\\n\\n### Backpropagation\\n- Calcula gradientes de la funci√≥n de p√©rdida\\n- Propaga errores hacia atr√°s\\n- Actualiza pesos con optimizador\\n\\n### Optimizadores\\n- SGD: Stochastic Gradient Descent\\n- Adam: Adaptive Moment Estimation (m√°s popular)\\n- AdamW: Adam con weight decay\\n- RMSprop: Para RNNs\\n\\nQUESTION: What is the attention mechanism in Transformers and how does it work?\\n</s>\\n<|assistant|>\\nThe attention mechanism in Transformers is called self-attention (attenci√≥n), which allows each token to attend to all other tokens in the input sequence by calculating a weighted sum of their own values and those of the corresponding keys. The formula for this operation is Attention(Q, K, V) = Softmax(QK^T / sqrt(d_k)) * V, where Q is the query vector, K is the key vector, and V is the value vector. This process helps to identify relevant information from the input sequence and focuses on the most important parts of the text. By attending to only the relevant elements, Transformers can improve the performance of various tasks such as language modeling, question answering, and machine translation. Overall, the attention mechanism plays a crucial role in transformer architectures, making them one of the most successful deep learning models currently available.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: Question about Transformers\n",
    "ask(\"What is the attention mechanism in Transformers and how does it work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c0e0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What are the advantages of using RAG instead of a pure LLM?\n",
      "\n",
      "üîÑ Processing...\n",
      "\n",
      "üí¨ Answer:\n",
      "============================================================\n",
      "<|system|>\n",
      "You are an expert assistant that answers questions based ONLY on the provided context.\n",
      "Answer concisely and accurately. If the information is not in the context, say \"I don't have enough information to answer\".\n",
      "</s>\n",
      "<|user|>\n",
      "CONTEXT:\n",
      "[data/rag_systems.txt]: # RAG - Retrieval-Augmented Generation\n",
      "\n",
      "RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje para crear sistemas que pueden responder preguntas bas√°ndose en documentos espec√≠ficos.\n",
      "\n",
      "## Por qu√© RAG?\n",
      "\n",
      "### Limitaciones de LLMs puros\n",
      "- Conocimiento desactualizado (fecha de corte)\n",
      "- No acceso a informaci√≥n privada/propietaria\n",
      "- Alucinaciones: inventan informaci√≥n falsa\n",
      "- No pueden citar fuentes espec√≠ficas\n",
      "\n",
      "[data/rag_systems.txt]: # RAG - Retrieval-Augmented Generation\n",
      "\n",
      "RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje para crear sistemas que pueden responder preguntas bas√°ndose en documentos espec√≠ficos.\n",
      "\n",
      "## Por qu√© RAG?\n",
      "\n",
      "### Limitaciones de LLMs puros\n",
      "- Conocimiento desactualizado (fecha de corte)\n",
      "- No acceso a informaci√≥n privada/propietaria\n",
      "- Alucinaciones: inventan informaci√≥n falsa\n",
      "- No pueden citar fuentes espec√≠ficas\n",
      "\n",
      "[data/rag_systems.txt]: ### Ventajas de RAG\n",
      "- Informaci√≥n actualizada en tiempo real\n",
      "- Acceso a documentos privados\n",
      "- Respuestas fundamentadas en fuentes\n",
      "- Reducci√≥n de alucinaciones\n",
      "- Trazabilidad y citaci√≥n\n",
      "\n",
      "## Arquitectura RAG\n",
      "\n",
      "QUESTION: What are the advantages of using RAG instead of a pure LLM?\n",
      "</s>\n",
      "<|assistant|>\n",
      "The advantages of using RAG instead of a pure LLM include:\n",
      "\n",
      "1. Current knowledge updates: RAG can access current knowledge about specific topics or domains through its retrieval system. This means that it can provide accurate responses even if the LLM has outdated information.\n",
      "\n",
      "2. Access to private and proprietary information: RAG does not require access to sensitive or proprietary information, which makes it more reliable for use in legal and regulatory settings.\n",
      "\n",
      "3. Accurate citations: RAG can cite sources from various sources, including academic journals, news articles, and government documents. This ensures that its responses are factually correct and backed by credible evidence.\n",
      "\n",
      "4. Real-time information: RAG can retrieve information as soon as it becomes available, making it ideal for situations where immediate action is required.\n",
      "\n",
      "5. Traceability and citation: RAG provides detailed traceability and citation information, allowing users to track the origins of their responses and verify their accuracy.\n",
      "\n",
      "Overall, RAG offers several benefits over pure LLMs, such as improved reliability, accuracy, and flexibility in response generation.\n",
      "============================================================\n",
      "\n",
      "üìö Sources used:\n",
      "   ‚Ä¢ data/rag_systems.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|system|>\\nYou are an expert assistant that answers questions based ONLY on the provided context.\\nAnswer concisely and accurately. If the information is not in the context, say \"I don\\'t have enough information to answer\".\\n</s>\\n<|user|>\\nCONTEXT:\\n[data/rag_systems.txt]: # RAG - Retrieval-Augmented Generation\\n\\nRAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje para crear sistemas que pueden responder preguntas bas√°ndose en documentos espec√≠ficos.\\n\\n## Por qu√© RAG?\\n\\n### Limitaciones de LLMs puros\\n- Conocimiento desactualizado (fecha de corte)\\n- No acceso a informaci√≥n privada/propietaria\\n- Alucinaciones: inventan informaci√≥n falsa\\n- No pueden citar fuentes espec√≠ficas\\n\\n[data/rag_systems.txt]: # RAG - Retrieval-Augmented Generation\\n\\nRAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje para crear sistemas que pueden responder preguntas bas√°ndose en documentos espec√≠ficos.\\n\\n## Por qu√© RAG?\\n\\n### Limitaciones de LLMs puros\\n- Conocimiento desactualizado (fecha de corte)\\n- No acceso a informaci√≥n privada/propietaria\\n- Alucinaciones: inventan informaci√≥n falsa\\n- No pueden citar fuentes espec√≠ficas\\n\\n[data/rag_systems.txt]: ### Ventajas de RAG\\n- Informaci√≥n actualizada en tiempo real\\n- Acceso a documentos privados\\n- Respuestas fundamentadas en fuentes\\n- Reducci√≥n de alucinaciones\\n- Trazabilidad y citaci√≥n\\n\\n## Arquitectura RAG\\n\\nQUESTION: What are the advantages of using RAG instead of a pure LLM?\\n</s>\\n<|assistant|>\\nThe advantages of using RAG instead of a pure LLM include:\\n\\n1. Current knowledge updates: RAG can access current knowledge about specific topics or domains through its retrieval system. This means that it can provide accurate responses even if the LLM has outdated information.\\n\\n2. Access to private and proprietary information: RAG does not require access to sensitive or proprietary information, which makes it more reliable for use in legal and regulatory settings.\\n\\n3. Accurate citations: RAG can cite sources from various sources, including academic journals, news articles, and government documents. This ensures that its responses are factually correct and backed by credible evidence.\\n\\n4. Real-time information: RAG can retrieve information as soon as it becomes available, making it ideal for situations where immediate action is required.\\n\\n5. Traceability and citation: RAG provides detailed traceability and citation information, allowing users to track the origins of their responses and verify their accuracy.\\n\\nOverall, RAG offers several benefits over pure LLMs, such as improved reliability, accuracy, and flexibility in response generation.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: Question about RAG\n",
    "ask(\"What are the advantages of using RAG instead of a pure LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d685210f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What optimizers are used to train neural networks?\n",
      "\n",
      "üîÑ Processing...\n",
      "\n",
      "üí¨ Answer:\n",
      "============================================================\n",
      "<|system|>\n",
      "You are an expert assistant that answers questions based ONLY on the provided context.\n",
      "Answer concisely and accurately. If the information is not in the context, say \"I don't have enough information to answer\".\n",
      "</s>\n",
      "<|user|>\n",
      "CONTEXT:\n",
      "[data/deep_learning.txt]: ### Regularizaci√≥n\n",
      "- Dropout: Desactiva neuronas aleatoriamente\n",
      "- Batch Normalization: Normaliza activaciones\n",
      "- Layer Normalization: Usada en Transformers\n",
      "- Weight Decay (L2): Penaliza pesos grandes\n",
      "\n",
      "## Frameworks\n",
      "- PyTorch: Flexible, preferido en investigaci√≥n\n",
      "- TensorFlow/Keras: Producci√≥n, TensorFlow Serving\n",
      "- JAX: Diferenciaci√≥n autom√°tica, XLA\n",
      "- Hugging Face: Modelos pre-entrenados de NLP\n",
      "\n",
      "[data/deep_learning.txt]: ### Regularizaci√≥n\n",
      "- Dropout: Desactiva neuronas aleatoriamente\n",
      "- Batch Normalization: Normaliza activaciones\n",
      "- Layer Normalization: Usada en Transformers\n",
      "- Weight Decay (L2): Penaliza pesos grandes\n",
      "\n",
      "## Frameworks\n",
      "- PyTorch: Flexible, preferido en investigaci√≥n\n",
      "- TensorFlow/Keras: Producci√≥n, TensorFlow Serving\n",
      "- JAX: Diferenciaci√≥n autom√°tica, XLA\n",
      "- Hugging Face: Modelos pre-entrenados de NLP\n",
      "\n",
      "[data/deep_learning.txt]: ### Transformers\n",
      "- Arquitectura dominante actual\n",
      "- Mecanismo de atenci√≥n (self-attention)\n",
      "- Procesamiento paralelo\n",
      "- Base de GPT, BERT, T5, etc.\n",
      "\n",
      "## Entrenamiento\n",
      "\n",
      "### Backpropagation\n",
      "- Calcula gradientes de la funci√≥n de p√©rdida\n",
      "- Propaga errores hacia atr√°s\n",
      "- Actualiza pesos con optimizador\n",
      "\n",
      "### Optimizadores\n",
      "- SGD: Stochastic Gradient Descent\n",
      "- Adam: Adaptive Moment Estimation (m√°s popular)\n",
      "- AdamW: Adam con weight decay\n",
      "- RMSprop: Para RNNs\n",
      "\n",
      "QUESTION: What optimizers are used to train neural networks?\n",
      "</s>\n",
      "<|assistant|>\n",
      "Optimizers used to train neural networks include:\n",
      "1. SGD: Stochastic Gradient Descent\n",
      "2. Adam: Adaptive Moment Estimation (AdaGrad, RMSProp)\n",
      "3. AdamW: Adaptive Moment Estimation with weight decay\n",
      "4. RMSprop: For RNNs\n",
      "\n",
      "The choice of optimizer depends on the specific problem being solved and the characteristics of the data being processed. The most commonly used optimizers for deep learning models are SGD and Adam.\n",
      "============================================================\n",
      "\n",
      "üìö Sources used:\n",
      "   ‚Ä¢ data/deep_learning.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|system|>\\nYou are an expert assistant that answers questions based ONLY on the provided context.\\nAnswer concisely and accurately. If the information is not in the context, say \"I don\\'t have enough information to answer\".\\n</s>\\n<|user|>\\nCONTEXT:\\n[data/deep_learning.txt]: ### Regularizaci√≥n\\n- Dropout: Desactiva neuronas aleatoriamente\\n- Batch Normalization: Normaliza activaciones\\n- Layer Normalization: Usada en Transformers\\n- Weight Decay (L2): Penaliza pesos grandes\\n\\n## Frameworks\\n- PyTorch: Flexible, preferido en investigaci√≥n\\n- TensorFlow/Keras: Producci√≥n, TensorFlow Serving\\n- JAX: Diferenciaci√≥n autom√°tica, XLA\\n- Hugging Face: Modelos pre-entrenados de NLP\\n\\n[data/deep_learning.txt]: ### Regularizaci√≥n\\n- Dropout: Desactiva neuronas aleatoriamente\\n- Batch Normalization: Normaliza activaciones\\n- Layer Normalization: Usada en Transformers\\n- Weight Decay (L2): Penaliza pesos grandes\\n\\n## Frameworks\\n- PyTorch: Flexible, preferido en investigaci√≥n\\n- TensorFlow/Keras: Producci√≥n, TensorFlow Serving\\n- JAX: Diferenciaci√≥n autom√°tica, XLA\\n- Hugging Face: Modelos pre-entrenados de NLP\\n\\n[data/deep_learning.txt]: ### Transformers\\n- Arquitectura dominante actual\\n- Mecanismo de atenci√≥n (self-attention)\\n- Procesamiento paralelo\\n- Base de GPT, BERT, T5, etc.\\n\\n## Entrenamiento\\n\\n### Backpropagation\\n- Calcula gradientes de la funci√≥n de p√©rdida\\n- Propaga errores hacia atr√°s\\n- Actualiza pesos con optimizador\\n\\n### Optimizadores\\n- SGD: Stochastic Gradient Descent\\n- Adam: Adaptive Moment Estimation (m√°s popular)\\n- AdamW: Adam con weight decay\\n- RMSprop: Para RNNs\\n\\nQUESTION: What optimizers are used to train neural networks?\\n</s>\\n<|assistant|>\\nOptimizers used to train neural networks include:\\n1. SGD: Stochastic Gradient Descent\\n2. Adam: Adaptive Moment Estimation (AdaGrad, RMSProp)\\n3. AdamW: Adaptive Moment Estimation with weight decay\\n4. RMSprop: For RNNs\\n\\nThe choice of optimizer depends on the specific problem being solved and the characteristics of the data being processed. The most commonly used optimizers for deep learning models are SGD and Adam.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3: Question about Deep Learning\n",
    "ask(\"What optimizers are used to train neural networks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cb95545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Explain the differences between supervised and unsupervised learning\n",
      "\n",
      "üîÑ Processing...\n",
      "\n",
      "üí¨ Answer:\n",
      "============================================================\n",
      "<|system|>\n",
      "You are an expert assistant that answers questions based ONLY on the provided context.\n",
      "Answer concisely and accurately. If the information is not in the context, say \"I don't have enough information to answer\".\n",
      "</s>\n",
      "<|user|>\n",
      "CONTEXT:\n",
      "[data/machine_learning.txt]: # Machine Learning - Fundamentos\n",
      "\n",
      "Machine Learning (ML) es una rama de la inteligencia artificial que permite a los sistemas aprender y mejorar autom√°ticamente a partir de la experiencia sin ser programados expl√≠citamente.\n",
      "\n",
      "## Tipos de Aprendizaje\n",
      "\n",
      "### Aprendizaje Supervisado\n",
      "El modelo aprende de datos etiquetados. Ejemplos:\n",
      "- Clasificaci√≥n: Spam detection, diagn√≥stico m√©dico\n",
      "- Regresi√≥n: Predicci√≥n de precios, pron√≥stico del tiempo\n",
      "\n",
      "[data/machine_learning.txt]: # Machine Learning - Fundamentos\n",
      "\n",
      "Machine Learning (ML) es una rama de la inteligencia artificial que permite a los sistemas aprender y mejorar autom√°ticamente a partir de la experiencia sin ser programados expl√≠citamente.\n",
      "\n",
      "## Tipos de Aprendizaje\n",
      "\n",
      "### Aprendizaje Supervisado\n",
      "El modelo aprende de datos etiquetados. Ejemplos:\n",
      "- Clasificaci√≥n: Spam detection, diagn√≥stico m√©dico\n",
      "- Regresi√≥n: Predicci√≥n de precios, pron√≥stico del tiempo\n",
      "\n",
      "[data/machine_learning.txt]: Algoritmos populares:\n",
      "- Linear Regression\n",
      "- Logistic Regression\n",
      "- Random Forest\n",
      "- Support Vector Machines (SVM)\n",
      "- Gradient Boosting (XGBoost, LightGBM)\n",
      "\n",
      "### Aprendizaje No Supervisado\n",
      "El modelo encuentra patrones en datos sin etiquetar:\n",
      "- Clustering: K-Means, DBSCAN, Hierarchical\n",
      "- Reducci√≥n de dimensionalidad: PCA, t-SNE, UMAP\n",
      "- Detecci√≥n de anomal√≠as: Isolation Forest\n",
      "\n",
      "QUESTION: Explain the differences between supervised and unsupervised learning\n",
      "</s>\n",
      "<|assistant|>\n",
      "In machine learning, there are two main types of learning: supervised and unsupervised. The difference lies in how the data is labeled or not labeled for training. In supervised learning, the model is trained with labeled data, which means it has been given specific labels for each example. This type of learning allows the model to learn patterns from the data without being explicitly programmed. Unsupervised learning, on the other hand, does not require any labeled data. Instead, the goal is to find patterns within the data that can be used to make predictions about new data. Unsupervised learning techniques include clustering, dimensionality reduction, and feature extraction. These methods allow the model to identify patterns in the data without being told what those patterns should be. Unsupervised learning is often used in areas such as natural language processing, image recognition, and recommendation systems. In contrast, supervised learning requires labeled data to train the model. This type of learning involves labeling examples with specific labels beforehand, allowing the model to learn patterns from the data. Examples of supervised learning techniques include regression, classification, and reinforcement learning. These methods help the model predict outcomes based on input features. Supervised learning is commonly used in fields such as finance, healthcare, and marketing. Overall, the choice between supervised and unsupervised learning depends on the nature of the problem at hand and the availability of labeled data. For some problems, supervised learning may be more appropriate while for others, unsupervised learning may be better suited.\n",
      "============================================================\n",
      "\n",
      "üìö Sources used:\n",
      "   ‚Ä¢ data/machine_learning.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|system|>\\nYou are an expert assistant that answers questions based ONLY on the provided context.\\nAnswer concisely and accurately. If the information is not in the context, say \"I don\\'t have enough information to answer\".\\n</s>\\n<|user|>\\nCONTEXT:\\n[data/machine_learning.txt]: # Machine Learning - Fundamentos\\n\\nMachine Learning (ML) es una rama de la inteligencia artificial que permite a los sistemas aprender y mejorar autom√°ticamente a partir de la experiencia sin ser programados expl√≠citamente.\\n\\n## Tipos de Aprendizaje\\n\\n### Aprendizaje Supervisado\\nEl modelo aprende de datos etiquetados. Ejemplos:\\n- Clasificaci√≥n: Spam detection, diagn√≥stico m√©dico\\n- Regresi√≥n: Predicci√≥n de precios, pron√≥stico del tiempo\\n\\n[data/machine_learning.txt]: # Machine Learning - Fundamentos\\n\\nMachine Learning (ML) es una rama de la inteligencia artificial que permite a los sistemas aprender y mejorar autom√°ticamente a partir de la experiencia sin ser programados expl√≠citamente.\\n\\n## Tipos de Aprendizaje\\n\\n### Aprendizaje Supervisado\\nEl modelo aprende de datos etiquetados. Ejemplos:\\n- Clasificaci√≥n: Spam detection, diagn√≥stico m√©dico\\n- Regresi√≥n: Predicci√≥n de precios, pron√≥stico del tiempo\\n\\n[data/machine_learning.txt]: Algoritmos populares:\\n- Linear Regression\\n- Logistic Regression\\n- Random Forest\\n- Support Vector Machines (SVM)\\n- Gradient Boosting (XGBoost, LightGBM)\\n\\n### Aprendizaje No Supervisado\\nEl modelo encuentra patrones en datos sin etiquetar:\\n- Clustering: K-Means, DBSCAN, Hierarchical\\n- Reducci√≥n de dimensionalidad: PCA, t-SNE, UMAP\\n- Detecci√≥n de anomal√≠as: Isolation Forest\\n\\nQUESTION: Explain the differences between supervised and unsupervised learning\\n</s>\\n<|assistant|>\\nIn machine learning, there are two main types of learning: supervised and unsupervised. The difference lies in how the data is labeled or not labeled for training. In supervised learning, the model is trained with labeled data, which means it has been given specific labels for each example. This type of learning allows the model to learn patterns from the data without being explicitly programmed. Unsupervised learning, on the other hand, does not require any labeled data. Instead, the goal is to find patterns within the data that can be used to make predictions about new data. Unsupervised learning techniques include clustering, dimensionality reduction, and feature extraction. These methods allow the model to identify patterns in the data without being told what those patterns should be. Unsupervised learning is often used in areas such as natural language processing, image recognition, and recommendation systems. In contrast, supervised learning requires labeled data to train the model. This type of learning involves labeling examples with specific labels beforehand, allowing the model to learn patterns from the data. Examples of supervised learning techniques include regression, classification, and reinforcement learning. These methods help the model predict outcomes based on input features. Supervised learning is commonly used in fields such as finance, healthcare, and marketing. Overall, the choice between supervised and unsupervised learning depends on the nature of the problem at hand and the availability of labeled data. For some problems, supervised learning may be more appropriate while for others, unsupervised learning may be better suited.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 4: Question about Machine Learning\n",
    "ask(\"Explain the differences between supervised and unsupervised learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e5604",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ed9eb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Interactive RAG System\n",
      "========================================\n",
      "Type your questions (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: what is a neuron ?\n",
      "\n",
      "üîÑ Processing...\n",
      "\n",
      "üí¨ Answer:\n",
      "============================================================\n",
      "<|system|>\n",
      "You are an expert assistant that answers questions based ONLY on the provided context.\n",
      "Answer concisely and accurately. If the information is not in the context, say \"I don't have enough information to answer\".\n",
      "</s>\n",
      "<|user|>\n",
      "CONTEXT:\n",
      "[data/deep_learning.txt]: # Deep Learning - Redes Neuronales Profundas\n",
      "\n",
      "Deep Learning es un subconjunto de Machine Learning basado en redes neuronales artificiales con m√∫ltiples capas.\n",
      "\n",
      "## Fundamentos\n",
      "\n",
      "### Neurona Artificial (Perceptr√≥n)\n",
      "- Recibe entradas ponderadas\n",
      "- Aplica una funci√≥n de activaci√≥n\n",
      "- Produce una salida\n",
      "\n",
      "[data/deep_learning.txt]: # Deep Learning - Redes Neuronales Profundas\n",
      "\n",
      "Deep Learning es un subconjunto de Machine Learning basado en redes neuronales artificiales con m√∫ltiples capas.\n",
      "\n",
      "## Fundamentos\n",
      "\n",
      "### Neurona Artificial (Perceptr√≥n)\n",
      "- Recibe entradas ponderadas\n",
      "- Aplica una funci√≥n de activaci√≥n\n",
      "- Produce una salida\n",
      "\n",
      "[data/deep_learning.txt]: ### Redes Convolucionales (CNN)\n",
      "- Especializadas en procesamiento de im√°genes\n",
      "- Capas convolucionales detectan patrones locales\n",
      "- Pooling reduce dimensionalidad\n",
      "- Arquitecturas: LeNet, AlexNet, VGG, ResNet, EfficientNet\n",
      "\n",
      "### Redes Recurrentes (RNN)\n",
      "- Procesan secuencias de datos\n",
      "- Mantienen estado interno (memoria)\n",
      "- Variantes: LSTM, GRU\n",
      "- Problema: Vanishing gradients en secuencias largas\n",
      "\n",
      "QUESTION: what is a neuron ?\n",
      "</s>\n",
      "<|assistant|>\n",
      "A neuron is a specialized cell in the nervous system of animals and humans that processes sensory input from other cells and transmits signals to other neurons or muscles for movement or behavior. It consists of a central core surrounded by several layers of membrane with ion channels and receptor proteins that allow electrical impulses to pass through it. The process of processing sensory inputs involves the activation of specific ion channels at the surface of the neuron, which can be triggered by different types of stimuli such as light, sound, touch, or chemicals. Once the signal has been processed, it is sent along the axon terminals to other neurons or muscle fibers for further processing or action. Overall, neurons play a critical role in the functioning of the brain and body, allowing us to perceive, understand, and respond to our environment.\n",
      "============================================================\n",
      "\n",
      "üìö Sources used:\n",
      "   ‚Ä¢ data/deep_learning.txt\n",
      "\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Interactive chat\n",
    "print(\"ü§ñ Interactive RAG System\")\n",
    "print(\"=\"*40)\n",
    "print(\"Type your questions (type 'exit' to quit)\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        question = input(\"\\n‚ùì Your question: \")\n",
    "        if question.lower() in ['exit', 'quit', 'q', 'salir']:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        if question.strip():\n",
    "            ask(question)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Goodbye!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff2e01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Conclusion\n",
    "\n",
    "### What We Built\n",
    "In this notebook, we implemented a complete **RAG (Retrieval-Augmented Generation)** system from scratch using:\n",
    "\n",
    "| Component | Technology | Cost |\n",
    "|-----------|------------|------|\n",
    "| **Vector Database** | ChromaDB | FREE |\n",
    "| **Embeddings** | Sentence Transformers (all-MiniLM-L6-v2) | FREE |\n",
    "| **LLM** | TinyLlama 1.1B (Local) | FREE & UNLIMITED |\n",
    "| **Orchestration** | LangChain | FREE |\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **RAG Architecture**: We learned how to combine retrieval and generation to create more accurate and grounded AI responses.\n",
    "\n",
    "2. **Vector Databases**: ChromaDB provides efficient similarity search for finding relevant document chunks.\n",
    "\n",
    "3. **Embeddings**: Sentence Transformers convert text to vectors that capture semantic meaning.\n",
    "\n",
    "4. **Local LLMs**: Running models locally on Colab's GPU eliminates API costs and rate limits.\n",
    "\n",
    "5. **LangChain**: Simplifies building complex AI pipelines with its modular components.\n",
    "\n",
    "### Advantages of This Approach\n",
    "\n",
    "- ‚úÖ **100% Free**: No API keys, no subscriptions, no hidden costs\n",
    "- ‚úÖ **Unlimited Usage**: No rate limits or quotas\n",
    "- ‚úÖ **Privacy**: Your data never leaves Colab's environment\n",
    "- ‚úÖ **Reproducible**: Everything runs in a single notebook\n",
    "- ‚úÖ **Extensible**: Easy to add new documents or swap components\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Add your own documents**: Upload PDFs, DOCX, or TXT files to the `data/` folder\n",
    "2. **Try larger models**: If you have more GPU memory, try Mistral 7B or LLaMA 2\n",
    "3. **Implement hybrid search**: Combine semantic search with keyword search (BM25)\n",
    "4. **Add reranking**: Use a cross-encoder to rerank retrieved documents\n",
    "5. **Build a UI**: Create a Gradio or Streamlit interface for better UX\n",
    "\n",
    "### Resources\n",
    "\n",
    "- üìö [LangChain Documentation](https://python.langchain.com/)\n",
    "- üìö [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- üìö [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
    "- üìö [RAG Paper (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You've built a complete RAG system that runs 100% free and unlimited on Google Colab!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
